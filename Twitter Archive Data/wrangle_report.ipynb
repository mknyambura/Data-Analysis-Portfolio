{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e2496c",
   "metadata": {},
   "source": [
    "# WeRateDogs Wrangling Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a81e53e",
   "metadata": {},
   "source": [
    "Mercy Nyambura Kariuki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce076a9",
   "metadata": {},
   "source": [
    "## Introdution\n",
    "\n",
    "Real-world data set rarely comes clean. The aim of this project is to wrangle WeRateDogs Twitter data and create logical analysis and visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c06ba69",
   "metadata": {},
   "source": [
    "The expected effort of my wrangling of this project consists of:\n",
    "- Gathering Data\n",
    "- Assessment\n",
    "- Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7bce6a",
   "metadata": {},
   "source": [
    "### Gathering Data\n",
    "\n",
    "Data is gathered from three sources.\n",
    "\n",
    "**Source 1: Enhanced WeRateDogs Twitter Archive\n",
    "\n",
    "- Download file twitter_archive_enhanced.csv manually from link provided, given to Udacity from @WeRateDogs. The WeRateDogs Twitter archive contains basic tweet data for all 5000+ of their tweets, but not everything. One column the archive does contain though: each tweet's text, which I used to extract rating, dog name, and dog \"stage\" (i.e. doggo, floofer, pupper, and puppo) to make this Twitter archive \"enhanced.\"\n",
    "\n",
    "**Source 2: Image Predictions File**\n",
    "\n",
    "- This file (image_predictions.tsv) is hosted on Udacity's servers and should be downloaded programmatically using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "\n",
    "**Source 3: Additional Data though the Twitter API\n",
    "\n",
    "- Twitter API : Query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called tweet_json.txt file\n",
    "\n",
    "*The recommended step to acquire this data should be query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called tweet_json.txt file.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4cc141",
   "metadata": {},
   "source": [
    "### Data Assessment\n",
    "\n",
    "After gathering each of the above data, assess them visually and programmatically for quality an tidiness.\n",
    "\n",
    "1. Drop records that are not dogs since we are only analysing data about dogs\n",
    "2. Filter tewwts to only have images in the `image_prediction` dataframe. Without images, tweets won't be useful in further analysis\n",
    "3. Filter `twitter_archive` to only have original tweets and remove retweets\n",
    "4. Drop columns that aren't required for further analysis e.g., `in_reply_to_status_id` ,`'in_reply_to_user_id` , `source` , `retweeted_status_id`, `retweeted_status_user_id`,`retweeted_status_timestamp` ,`expanded_urls`,`name`\n",
    "5. Remove \"None\" values in dog stage and replace with a blank text\n",
    "6. Merge Dog stage columns `doggo`, `floofer`, `pupper`, `puppo` in `twitter_archive_clean` in one column called `Stage` ---> one variable should be represented in one column to be **tidy** then drop them as it  they won't be needed for further analysis\n",
    "7. Fix inconsistent `rating_denominator` and `rating_numerator` values\n",
    "8. Remove unnecessary columns in `image_preditions`\n",
    "\n",
    "### Cleaning data\n",
    "\n",
    "Before we start cleaning, we'll copy the orginial dataframes to new dataframes\n",
    "\n",
    "The cleaning process involves:\n",
    "- **Define:** These definitions serve as an instruction list so others (in the future) can look at the work and reproduce it.\n",
    "- **Code:** Convert the  above definitions to code and run that code\n",
    "- **Test:** Test the code to make sure the cleaning operation has worked.\n",
    "\n",
    "### Conclusion\n",
    "The wrangled data is stored in **master_WeRateDogs.csv** which is then ready to be analyzed to draw useful insight from it, then to build visualization and reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d124ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
